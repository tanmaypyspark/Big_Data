############Azure Databricks Mastery: Hands-on project with Unity Catalog , Delta lake, CI/CD implementing Medallion Architecture##################

###Azure Databricks - An Introduction
	#Big data approach:
		1. Monolithic: Single Computer for Data Storage and Processing
		2. Distributed: Adding multiple machine to achieve parallel processing
	
	#Drawbacks of MapReduce:
		1. Traditional Hadoop mapReduce processing:
			HDFS Disk ---HDFS Read --> Iteration 1 (Process Data) --HDFS Write --> Storage(HDFS Disk) --HDFS Read--> Iteration 2 (Process Data) --HDFS Write --> Storage(HDFS Disk)
			 A. so for every iteration it used to perform the read and write from disk. this will cause high disk inputs and outputs
			 B. As developers has to code for map and reduce function, which is also very complex to implement, Thay also maintain the mapping logic and the reducing logic,
				which is tough and limited.
		This tends to introduce spark
	#Enhance of Spark:
		HDFS or Any cloud storage --HDFS Read--> Iteration 1(RAM) [Analysis Data] ---> Iteration 2 (RAM) [Analysis Data]
	Apache Spark provides primitives for in-memory cluster computing.
	A spark job can load and cache data into the memory and query it repeatedly.The in-memory computing is much faster than disk based application.
	
	#Apache Spark:
		is an open source in-memory application framework for distributed data processing and iterative analysis on massive data volumnes.
		 * In simple terms, Spark is a :
			a. Compute Engine: Provides fast, in-memory distributed computation without being coupled to any storage system, which means here the compute which is the memory and the storage are decoupled.
	#Apache Spark Ecosystem:
	
	######################################################################################
	#              # # Spark	  # #          # #                     #  #              #
 	#              # # Streaming  # # Spark ML # # Spark Graph         #  # SparkR       #
	#              # #            # # Mlib     # # (Graph Computation) #  # (R on Spark) #
	# Spark SQL    # #			  # #          # #                     #  #              #
	#(Interactive) # #####################################################################
	#              # #                                                                   #
	#              # #                                                                   #
	#              # #  DataFrame/ Dataset APIs                                          #
	#              # #                                                                   #
	#              # #                                                                   #
	######################################################################################

					###########################################################                         
					#						Sparke Core						  #                         
					#                                                         #                         
					#		######### ########## ########## ####### ######	  #                         
					#		# Scala # # Java #   # Python # # SQL # # R  #    #                         
					#		######### ########## ########## ####### ######    #                         
					#       RDD - Resilient Distributed Dataset               #                         
					###########################################################    
	###########################################################################################
    #       Distributed Compute Engine :- Spark Engine										  #
	###########################################################################################
	An operatiing system which will manage a group computers that is called cluster Manager.
	
	Q. What is Databricks:
		Founder of spark create a commercial product, called as Databricks. Ehich gives a cloud platform to work with Apache Spark which is in a more 
		efficient way with its runtime.
	
	Q. What is Azure Databricks?
	A. 
	   1. Unified interface: Spark will provide a unified interface, a single interface where you can manage your data engineering,science, analytics operations.
	   2. Open analytics platform
	   3. Compute Management
	   4. Notebooks
	   5. Intergrates with cloud storages
	   6. MLFlow modeling
	   7. Git
	   8. SQL Warehouses
	
	#Azure Databricks Architecture:
		It's structured in a way where you can focus on your core data engineering or data science or data analyst taksks.
	Q. What resources will be managed by cloud service provider and managed by Databricks?
	A. In Azure Databricks services there are 2 plane :
		1. Control Plane (Databricks) [Includes the back end services that databricks managed][Notebooks, commands, Jobs, Queries]
		2. Compute Plane (Azure) [To run your notebook, will create clusters, Cluster will be actually present in the compute plane]
		But cluster manager who takes the configuration to create the cluster will be under the control plane.
		Summarization: the resourcces that databricks provides to use to work with spark will be under the control plane, it mostly contains the metadata information such aws jobs, cluster manager, the notebook names and command inside the notebook.
		
	Control Plane: Databricks web Application, Notebooks, Jobs & Queries, Cluster Manager
	Compute Plane : Azure Datalake Gen2, vNet(Virtual Machines)
	
	# Cluster Types and Configuration:
		Cluster is a set of computer working as single machine run your workloads.
		* Workloads can be:
			1. set of commands in a notebook
			2. A job that you run as a automated workflow
		*Cluster Types:
			1. All purpose cluster:
				* To execute set of commands in a notebook Generally used when we want to interactively work with our notebook.
				* Multiple users can share such cluster to do collaborative interactive analysis.
				* You can terminate, restart, attach, detach these clusters to multiple notebooks(dash)
				* You can choose:
					@ Multiple-node cluster = Driver node and executor nodes will be on separate machine.
					@ Single-node cluster = Only there will be a single driver node with single machine
			
			2. Job Cluster:
				* To execute a job that run as a automated workflow.
				* It runs a new job cluster and terminates the cluster automatically when the job is complete.
				* You cannot restart a job cluster
	#Cluster Access Mode:
		Is a security feature that determine who can use a cluster and what data they can access via the cluster.
		
		There are 3 types pf access mode:
		Id, Access Mode, Visible to user, UC Support, Supported Language, Notes
		1. Single User, Always, Yes, Python/SQL/Scala/R, Can be assignes to and used by a single user.
		2. Shared, Always(Premium Plan or above required),Yes, Python(on DB Runtime 11.1and above)/SQL/Scala(on unity catalog enabled clusters using DB Runtime 13.3 and above),Can be used by multiple users with data isolation among users.
		3. No Isolation Shared, Yes Admins can hide this cluster types by enforcing user isolation in the admin setting page, No,Python/SQL/Scala/R, There is a related account-level setting for No Isolation Shared cluster.
	
	###########################################################################################################################################################
	#                                                                                                                                                         #
	#															DELTA LAKE																					  #	
	#                                                                                                                                                         #
	###########################################################################################################################################################
	
	## What is delta lake?
	A. 1. Open source storage framework that brings reliability to data lakes
	   2. Brings transation capabilities to data lakes
	   3. Runs on top of your existing datalake and supports parquet
	   4. Enables Lakehouse architecture
	## How to create delta lake?
	A. using parquet formate we can't update the data directly, we have to use delta format
		df.write.format('delta').save(path)
	 Create file part*.parquet and log folder--> trmp_path, crc file and json file
	 Read delta file:
		df.read.format('parquet').load(path)
		if we use this through error  "Incompatible format detected'
	##Understanding the delta format
	A. 
	  Delta format --> under the parquet+transation log (delta log folder)
	  delta lake build top of data lake(ADLS)
	  
	  delta/ 1. _delta_log/
				1. 0000.json ( Containes transation information applied on actual data)
				2. 0001.json
			 2. Partition directory (if applied)
				1. file01.parquet (containes actual data)
			Note: Delta log folder is metadata cache folder
	## Understanding transation Log:
	a.
		In .json file all the details of delta file mentions like number of rows, who perform that action, when that actions performed
		when ever we over write the delta file it creates a new .parquet file and transation log files to mantain the SCD types.
		But in parquet format if we overwrite that it remove the old file and create a new file, it folows SCD 0 data type.
		
		# At the time of read from delta folder:
			When ever we read any delta file,spark first check the transation log .json file, and take the latest timestamp file 
		QQ. when we overwrite a file it loggically remove the file from transationlog json file but why it still present in ADLS?
		AA. Bez time-travel/versioning feature in delta lake
	
	## Creating delta tables using SQL
	-->
		$ In order to create any table we have to create schema:
			CREATE SCHEMA IF NOT EXISTS delta
			# Here schema is nothing but database. If you want check the db click catalog--> click databricks name or default
		$ Create a table:
			For best practice use 2 level name space(using ``):
				%sql
				--CREATE TABLE
				CREATE OR REPLACE TABLE `delta`.deltaFile
				(
				  Education_Level VARCHAR(50),
				  Line_Number INT,
				  Employed INT,
				  Unemployed INT,
				  Industry VARCHAR(50),
				  Gender VARCHAR(10),
				  Date_Inserted DATE,
				  dense_rank INT
				)
		Note: Any table creating by default is a delta table in the databricks, as we are not defining any location this is also treated as Managed table.
	
	##Create delta table using PySpark:
		df.write.format('delta').saveAsTable('`delta`.delataSpark')
	
	##Schema Enforcement:
	-->
		Schema is blue print of data that define the shape of data, column, meta data
		Ex:
			let's take a delta table which is having 4 column(col1,col2,col3,col4),here table structure and table schema is maintained strictly. we are ingesting data into this table on daily basic.
			Now on a fine day during the data ingestion some data comes with a new column which is not in the schema of our current table, which is being overwritten to the location where our delta lake is present.
			So if we use data lake (not delta lake), it's allow the overwrite of this data and we will loss any our original schema.
			But for delta lake we have a feature called schema enforcement or schema validation, which will check for the schema for whatever the data that is getting written on the delta lake table.
			if the schema does not match with the data which we are trying to write to the destination, it's going to reject that particular data.It will cancel the entire write operation and generates an error stating that the schema is not matching
			the module of the schema.Validation is to safeguard the delta lake that ensures the data quality by rejecting the writes to a
			table that do not match the table schema.
			
			@@There are certain rules on how the schema enforcement works:
				Schema Enforcement Rules:
					1. Cannot contain any additional columns that are not present in the target table's schema.
					2. Cannot have column data types that differ from the column data types in the target table.
			Note: If we want to ingest data frequently we have to use .mode('append')
				  If we want to load/ append the data which is having more coles it throughes the error ' A schema mismatch detected when writing to the Delta table'
				  But if we want to load/append with less column it works fine, for missing columns it assgin as null values.
			Note:
				If we want to upload the data which is having different data types it through error '[DELTA_FAILED_TO_MERGE_FIELDS] Failed to merge fields 'Line_Number' and 'Line_Number''
	##Schema Evolution:
		It's a feature to add a new column in the existing table, which is not possible in schema enforcement.
		Example:
		there is 2 way to do this:
		1. .option('mergeSchema','true')
			like :
					df_more.write\
					.format('delta')\
					.option('mergeSchema','true')\
					.mode('append')\
					.saveAsTable('`delta`.deltaspark')
		
		2. spark.databricks.delta.schema.autoMerge.enabled = true
		
		##Allow different schema to evolve:
			df.write.format('delta').option('overwrite').option('overwriteSchema','true')\
				.saveAsTable('`delta`.deltaspark')
	
	##Audit Data Changes & Time Travel (Time Travel and Versioning)
	--> 
		* Delta automatically versions every operation that you perform
		* You can time travel to historical versions
		* This versioning makes it easy to audit data changes, roll back data in case of accidental bad
		  writes or deletes, and reproduce experiments and reports.
		
		#Access old version:
			#Using PySpark
			spark.read.format('delta')\
				.option('versionAsOf','1')\
				.load(path)
			
			#Using SQL:
				SELECT * FROM `delta`.versionTable VERSION AS OF 3 where Industry ='Networking'
			
			#using @v (VersionNumber) after Table Name:
				#PySpark:
					spark.read.format('delta')\
					  .load(path+'@v1')
				#SQL:
					select * from DELTA.`path@v1`
			#Using TimeStampAsOf:
			    #PySpark:
					spark.read.format('delta')\
					.option('timestampAsOf','2023-12-08T05:06:44Z')
				#SQL:
					SELECT * FROM `delta`.versionTable TIMESTAMP AS OF "2023-12-08T05:06:44Z" where Industry ='Networking'
			
			Note: If we use timestamp as '2023-03-12' it through error like 'deltaErrorBase.TIMESTAMPEarlierTahanCommitRent..
	
		#Restore the table:
			#Using RESTPRE commands:
				RESTORE TABLE `delta`.versiontable TO VERSION AS OF 1
	
	##Vacuum Command:
	--> 
		Vacuum command help us to delete files if they are not valid. Delete old deltes
		
		# How many files will be deleted we can use dry run:
			%sql
			VACUUM delta.vactable DRY RUN
			
			#it's not going to delete any kind of data it's only showing the list of files are gonna delete
			by deafult retention period is more 7 days.
		
		#we can change retention days,time
		--> %sql
			VACUUM `delta`.Vactable RETAIN 5 HOURS 
			Note: data above the 5 hourse got deleted.
		When ever we try to run this comman it's showing error:
		IllegalArgumentException: requirement failed: Are you sure you would like to vacuum files with such a low retention period? If you have
		writers that are currently writing to this table, there is a risk that you may corrupt the
		state of your Delta table.

		If you are certain that there are no operations being performed on this table, such as
		insert/upsert/delete/optimize, then you may turn off this check by setting:
		spark.databricks.delta.retentionDurationCheck.enabled = false

		If you are not sure, please use a value not less than "168 hours".
			   
		state of your Delta table.
		If you are certain that there are no operations being performed on this table, such as
		insert/upsert/delete/optimize, then you may turn off this check by setting:
		
		spark.databricks.delta.retentionDurationCheck.enabled = false
		
		If you are not sure, please use a value not less than "168 hours".
		
		Note: Not recommanded to use vaccum command in developer or Prod enviroment, it's going to delete old files.
	
	## Convert To Delta:
		We can convetr a parquet file into delta file at the same location.
		suppose we have a parquet file which is processed in a location
		let parquet dataframe is parquet_df and location is final_path:
		
		#Using sql:
		CONVERT TO DELTA parquet.`target_path`
		
		#Using PySpark:
		from delta.tables import DeltaTabel
		
		DeltaTable.convertToDelta(spark,final_path)
		
		#Check the history:
		#using SQL:
		DESCRIBE HISTORY delta.`path`
		#using PySpark:
		deltaTable = DeltaTable.forPath(spark,final_path)
		
		fullHistoryDF = deltaTable.history()
		lastOperationDF = deltaTable.history(1)
	
	## Optimize in Delta Lake:
		The optimize command will help us to compact multiple small files into a single	file.
		So this is one of the optimization feature in the delta lake which the name itself indicates the optimize.
		
		Now the use of this particular command is to compact multiple files into a single file.
		If you are aware of the small file problem in the spark, where if you have 100 small files, 
		where each transaction is creating a file(json log), if you want to read the content of each and every file, the time
		to open each and every file is more than reading the actual file.	It need to open the file, it need to read the content, and it need to close the file.
		So the time to open and close each and every file is more than actual content reading.So this is why the optimize command helps in
		a way where it can just combine all the active files into a single file.
		
		So let's see by taking an example:
			So we'll be doing some transformations on our data in our Delta lake transformations are nothing but the operations like inserts, deletes and updates and etc..
			So each action or a transformation will be treated as a commit and it will create the parquet file. Along with that it will create the delta log files.
			
			So imagine we are creating a empty table because we are doing an empty table creation.
			It is also an operation where the operation is recorded as a create table.
			And it is not going to create any parquet file, but it is going to create a delta log.
			So it is going to create a 000. Json file.
			
			Now this indicates that a create table action has been performed.
			
			Now we are going to do three write operations where I am sequentially writing three inserts.
			
			Now as each insert will have a data, it is going to write three parquet files.
			So for easy understanding I am just naming the files as AB parquet, CD dot parquet and then goes on
			in alphabetical order. So each insert will make a parquet file and a Json file are also created parallelly.
			
			If you are trying to apply the optimize command on top of it, it need to only pick the active files,
			not the inactive files.
			So inactive files were logically considered as they are removed, but they are still present in the
			system so that if you can do the time travel, it can bring it back.
			Now it is going to pick only the active files, and it is going to combine it as a single file.
		
		##Optimize Command:
			INSERT:
				1. "add":{"path":"part-00000-258a5518-0736-4095-9749-ef02d183a68b-c000.snappy.parquet"  -- Active
				2. "add":{"path":"part-00000-75e27b90-364b-4e90-98bd-fa9654599554-c000.snappy.parquet", -- Inactive
				3. "add":{"path":"part-00000-7d196389-b51c-44bf-ba29-b759dd3caa6d-c000.snappy.parquet"  -- Inactive
			DELTE Line Number 104:
				"remove":{"path":"part-00000-75e27b90-364b-4e90-98bd-fa9654599554-c000.snappy.parquet"
			Update a record:
				{"remove":{"path":"part-00000-7d196389-b51c-44bf-ba29-b759dd3caa6d-c000.snappy.parquet"
				"add":{"path":"part-00000-b4b0bffb-2428-4051-aac0-ba8bf32dd69a-c000.snappy.parquet" 	--Active
		##Optimize:
			optimize `delta`.optimizetable
			
			it will remove all the active file and combine it in 1 single file:
				remove:
					"remove":{"path":"part-00000-258a5518-0736-4095-9749-ef02d183a68b-c000.snappy.parquet"
					"remove":{"path":"part-00000-b4b0bffb-2428-4051-aac0-ba8bf32dd69a-c000.snappy.parquet"
				ADD:
					"add":{"path":"part-00000-6edab5b0-d8a2-4e20-9b3a-79580aa3bf5d-c000.snappy.parquet"
		#Note: Versioning is also working in this table
	
	##UPSERT (MERGE) In Delta Lake:
		Merge is nothing, but it is going to combine the data where we will be using that to update
		the values or insert the values.
		So we need to have a common column where we will have that as a reference.
		So taking that column as a reference in both the sides source side and the destination side, if ever
		there is a match in that particular column which is in the incoming data, you need to update that column.
		If ever there is no common column found in the incoming data, you need to insert that.
		
		So Upsert is nothing but UPDATE + INSERT
		
		* UPSERT = UPDATE + INSERT data using merge command
		* If any matching rows found, it will update them
		* If no matching rows found, this will insert that as new row
		
		MERGE INTO < Destination_Table>
		USING <Source_Table>
		ON <Dest>.col2 = <Source>.col2
		WHEN MATCHED
		THEN UPDATE SET
			<Dest>.col1 = <Source>.col1,
			<Dest>.col2 = <Source>.col2
		WHEN NOT MATCHED
		THEN INSERT
			VALUES(source.col1,source.col2)
			
		#Code
		%sql
		MERGE INTO `delta`.dest_table as dest
		USING source_table as src
		  ON dest.Line_Number = src.Line_Number
		  WHEN MATCHED
			THEN UPDATE SET
		  dest.Education_Level = src.Education_Level,
		  dest.Line_Number = src.Line_Number,
		  dest.Employed = src.Employed,
		  dest.Unemployed = src.Unemployed,
		  dest.Industry = src.Industry,
		  dest.Gender = src.Gender,
		  dest.Date_Inserted = src.Date_Inserted,
		  dest.dense_rank =src.dense_rank
		WHEN NOT MATCHED
		THEN INSERT 
		(Education_Level, Line_Number, Employed, Unemployed, Industry, Gender, Date_Inserted, dense_rank
		  )
		  VALUES(src.Education_Level, src.Line_Number,src.Employed, src.Unemployed, src.Industry, src.Gender,src.Date_Inserted,src.dense_rank)
		
		@ What is the purpose of the Databricks Delta in Azure Databricks??
		A. To Version control and manage data consistency in data lakes.

		@ In Delta Lake, what is the purpose of the Delta table's transaction log, and how does it contribute to data consistency?
		A. The transaction log tracks all changes to the delta table, ensuring ACID properties (Atomicity, Consistency, Isolation, Durability).

		@ What is the significance of the OPTIMIZE command in Databricks Delta Lake, and how does it contribute to performance optimization?
		A. OPTIMIZE reduces the number of small files and compacts data, improving query performance and storage efficiency.

		@ In Delta Lake, what role does the concept of "time travel" play, and how does it benefit users in data exploration and analysis?
		A. "Time Travel" allows users to go back in time and revert changes made to the Delta table for data correction.

		@In Databricks Delta Lake, what does "schema evolution" refer to, and how does it contribute to managing changes in data structures over time?
		A. Schema evolution allows for the addition, removal, or modification of columns in a Delta table without requiring a full table rewrite.

###########################################################################################################################################################################
																	UNITY CATALOG                                                                                         #
###########################################################################################################################################################################

	@ What problem solvedby unity catalog?
	
		Azure Databricks Workspace
					|
					|
			User Management
					|
					|
			Hive Metastore
					|
					|
			 Acess Control
					|
					|
		 Cluster, SQL Warehouses
					|
					|
			Azure Datalake Gen2
	
	################################################Databricks Unity Catalog########################################
	#
	# Access Control, Lineage, Discovery, Monitoring, Auditing, Sharing
	################################################################################################################
	#
	#   Metadata management
	#	(Tables | Notebooks | Dashboards)
	##############
	
	## Creating Access Connector for Databricks
	--> Accesss connector --> create--> go to adls--> Access Control (IAM)--> ADD --> role assignment--> Storage Blob Data Contributor --> Members(Maanged Identity) --> select members-->Review+Assign
	
	## Create Metastore inUnity Catalog:
	--->
		Metastore:
			A metastore is the top-level container for catalog in Unity Catalog. Within a metastore, Unity Catalog provides a 3-level namespace for organizing data: catalogs, schemas (also called databases), and tables / views.
			
		Managed Account --> Catalog-->Create Metastore
		adlsgen2 --> abfss://container@storageaccountname.dfs.core.windows.net/foldername
		Access Connector Id --> access_connector(Resource ID)
		Note: Only a single metastore per region is allowed.
	
	#Enable Unity Catalog?
	A.
	 @ Assigning the metastore will update workspaces to use Unity Catalogmeaning that,
		* Data can be governed and acess across workspaces.
		* Data acess and lineage is captured automatically.
		* Identities are managed centrally in the account console.
	 @ Before enabling Unity Cataog, consider these readliness check:
		* Understand the privileges of workspace admins in unity catalog and review existing workspace admin designations.
		* Update any automation for principle/ group management, such as SCIM, Okta and AAD connectors, and Terraform to reference account endpoints instead of workspace endpoints
		* Be aware that this action cannot be reversed
	
	##Unity Catalog Object Model:
	--->
						 Metastore
							 |
							 |
	Storage Credential,Catalog , External Location, Share, Recipient, Provider
							 |
							 |
						  Schema
							 |
							 |
					Table |  View | Function | Volume | Model
	
	Note: Unity catalog tables are delta tables
	
	#################
	##Role in Unity Catalog:
	-->
		Account Admin:
				1. Create metastore and link workspaces
				2. User and Group Management
				3. Billing and Cost
		Metastore Admin:
				1. Create and manage Catalogs
				2. Create and manage external locations
		Workspace Admin:
				1. Create and manage workspaces
				2. Create and manage clusters
		Workspace Users:
				1. Can create tables, schemas, Objects
	##Creating Users in Azure Entra ID:
	#Create sample users
		 Click side bar--> Microsoft Entra ID--users--New User--Create a new user
			--> Review+Create
		Pw: 123456789@Tm12/9932843949@Tm
		username: Tony@tanmaymazure1gmail.onmicrosoft.com
		
		shyam@tanmaymazure1gmail.onmicrosoft.com
		
	## Users And Group Management:
	 * Invite and add users to Unity Catalog
	 * Create groups:
		* Workspace admins
		* Developers
	* Assign groups to users"
		* Workspace admins --> Tony@tanmaymazure1gmail
		* Developers --> Shyam@tanmaymazure1gmail
	* Assign roles to groups:
		* Workspace Admin --> Workspace Admins Group
		* Workspace User --> Developers Group
	
	##Cluster Policies:
	--> use to create rules for cluster for developers.
		1. TO control user's ability to configure clusters based on a set of rules.
		2. These rules specify which attributes or attribute values can be used during cluster creation.
		3. Cluster policies have ACLs that limit their use to specific users and groups
		4. A user who has unrestricted cluster create permission can select the Unrestricted policy and create fully-configurable clusters.
	
		We can fixed all the values there so developer can't change those.
	
	##What are cluster pools:
	--> 
		Databricks Pool will already place a request to cloud service provider before we create any cluster to use them.
		And the cloud service provider will give the required virtual machines based on the configuration that 
		we can keep one virtual machine in running state and other virtual machines can be in idle.  
		The cluster creation time will be reduced by more than half while keeping this Databricks pool.
		
		This feature comes with cost..as one VM is in running state always.
		
		With out cluster pool:
		When ever we request for cluster databricks asck azure cloud to create vm that time.
	##Creating a Cluster pool:
		DB-->Compute-->Create Pool --> assign min number of 
		
		Diff b/w --> All On-demand vs All spot:
		On-demand instances have a fixed price and are always available, 
		while spot instances operate on a bidding system and are not guaranteed to be available.
		
		With Instance pool--> takes <1.30 min
		without Instance pool --> >5 min
	## Creating a Dev Catalog:
	--->
					 Catalog		SCHEMA      TABLE
		Metastore --> 1. Pl_Dev  --> 1. Bronze --> Table
 					  2. Pl_UAT		 2. Silver --> Table
					  3. Pl_Prod	 3. Gold  ---> Table
	##Unity Catalog Privileges:
	-->
		* Privileges are permission that we assign on objects to users.
		* Can use SQL command or Unity Catalog UI
			Eg:
				GRANT privilege_type ON securable_object TO principal
					privilege_type : Unity Catalog permission like SELECT,CREATE
					securable_object: Any object like SCHEMA, TABLE, etc
					principal: CAn be a user,group,etc
	
	#Create a catalog:(login from owner account)
		side bar--> catalog --> create catalog--> permission(to give permission to admin group, developer group){grant,revoke}
		# We can use SQL to grant or revoke permission to users,group,etc:
			%sql
			GRANT USE_CATALOG ON CATALOG `dev_catalog` TO `Developers G`
			
		By default if we are not giving any name table will create under hive_metastore.
		Always use three level Namespace:
			SELECT * FROM `catalog`.`schema`.Table
			
			Metastore --> catalog(level 1) --> Schema(level 2)--> Table/view/Function(level3)
	
	##Understanding Unity Catalog:
	--->
			###############################						############################
			# Databricks Access Connector # --------------------# Azure Databricks Service #
			###############################						############################
						|
						|
						|
						|
			#######################
			# Azure Datalake Gen2 #
			#######################
						Storage Credential     	 			  Vs  					External Location
		1. An authentication and authorization mechanism for      1. Serves as a reference point for External storage
		   accessing data stored.
		2. Stores the access Credentials to provide access to     2. Stores the path of the external storages that you want to
		   External Location										 access.
		3. Credentials can be Managed Identities / Service        3. Makes use of storage credential to get access to External Storage.
			principal 
	
################################################################################################################################################################################
#																		SPARK STRUCTURED STREAMING																			   #
################################################################################################################################################################################

	## Spark Structured Streaming:
		#Read:
			df_r_stream = spark.readStream.format('csv').option('header','true').schema(schema).load(stream_path)
			Note: Spark is not going to read any specific files rather it read from a folder
		#Display:
			df_r_stream.display()
		
		Note: 1. When we read any data from the stream folder there will be no job untill or unless we display that dataframe
			  2. When any data comes to that folder that is going to read as micro batch
		
		Q. How stream knows that there is a new file available?
		A. In the backend there is a particularly streaming query.
		Q. How spark know that there a data available and it needs to create a job?
		A. If we use display() it is running in background to generate the job.
		
		Q. What are the supported soucres and sink for streaming?
		A. Sources:
				1. File source
				2. Kafka Source
				3. Socket Source
				4. Rate Source
				5. Table
				
				#Socket and Rate for testing not for Production
			Sink:
				1. File Sink
				2. Kafka Sink
				3. Foreach Sink
				5. Console Sink
				6. Table
	
	## StreamWriter:
		To write the stream we have to use:
			df_stream.writeStream.\
				.option('checkpointLocation',<Location>)\
				.outputMode('append')\
				.toTable('<Table>')
		
		#CheckpoinyLocation: It is use to store the progress of our stream, like the metadata till where the data is copy.
			* To develop fault-tolerant an resilient Spark applicaction
			* It maintains intermediate state on fault-tolerant compatible file system like HDFS,ADLS and S3 storage systems to recover from failures.
			* Must be unique to each stream.
		
		#outputMode:
			Append --> outputMode('append') --> The records from incomming streams will be appended to destination
			Complete --> outputMode('complete')--> All the processed rows will be displayed
			Update --> outputMode('update')--> Spark will output only updated rows. This is valid only if there are aggregation results; otherwise, this would be similar to Append mode.
			
			##Use Append:
				When we restart the cluster and write the stream data at same path and same table...and query that table it's showing no results bez it check the checkpointLocation meta data
				as there is no new input it doest not append the data.\
				In order to process the old data we need to delete the checkpointLocation.
				delete checkpointLocation --> dbutils.fs.rm('/mnt/Streaming/raw/AppendCHECKPoint',True)
				
				df_Streamwrite = df_r_stream.writeStream\
							.option('checkpointLocation',f'{stream_path}/AppendCHECKPoint')\
							.outputMode('append')\
							.queryName('AppendQuery')\
							.toTable('hive_metastore.stream.AppendTable')
			
			##Use Complete:
				Note:  Complete output mode not supported when there are no streaming aggregations on streaming DataFrames/Datasets;
				Complete mode is use when we do some Aggregation on writeStream
				
				df_StreamwriteC = df_complete.writeStream\
				.option('checkpointLocation',f'{stream_path}/CompleteCHECKPoint')\
					.outputMode('complete')\
					.queryName('CompleteQuery')\
						.toTable('hive_metastore.stream.CompleteTable')
	
	##Understanding Triggers:
		Trigger are used to kick off the streaming:
		
		1. Default Trigger:
			If you are not specify any trigger, it's a default trigger, It execute stream query every 5-6 sec interval.
			df_writeStream = (df_r_stream.writeStream
						.option('checkpointLocation',f'{stream_path}/AppendCHECKPoint')
						.outputMode('append')
						.queryName('DefaultTrigger')
						.toTable('hive_metastore.stream.AppendTable'))
			It's keep on running unless manualy stop this query. There is a disadvantage here: If our source is performing append data in every 5-10 min there is unnessery
			check the data every 5-6 sec.
		2. Processing Time:
			Here we can mention the interval of query execution. It means we can mention the processTime so after that mentioned time trigger will kick off the query.
			Here 2 scenarios are come:(Suppose we gave processTime = '2 minutes')
				1. And data is not come after 2 min then it'll wait till the data is comming.Once data arrived it kick off the query.
				2. Suppose 1 micro batch is taking long time to execute the transformation (more that processTime) then after completed the query, it immediatly start the next query.
				
				df_writeStream = (df_r_stream.writeStream
						.option('checkpointLocation',f'{stream_path}/AppendCHECKPoint')
						.outputMode('append')
						.trigger(processingTime='2 minutes')
						.queryName('ProcessingTimeTrigger')
						.toTable('hive_metastore.stream.AppendTable'))
		3. Availablenow:
			This trigger automatically stop once the process is done. This will check any available data from previous micro batch and this will process all the available data after previoud batch
			and it'll stop the streamming query once it's processs. This will work like batch processing with in incremental Load.
			We need to use some external schedular to exexute this query.
			
			df_writeStream = (df_r_stream.writeStream
						.option('checkpointLocation',f'{stream_path}/AppendCHECKPoint')
						.outputMode('append')
						.trigger(availableNow=True)
						.queryName('AvailableNow')
						.toTable('hive_metastore.stream.AppendTable'))
		Note: There is 1 more trigger continousTrigger, which is stream for mili second interval..which is not available.
		
		
		Default Trigger--> --> Will trigger the micro batch for every 500 ms or half a second.
		processingTime(Fixed Interval) --> trigger(processingTime='2 minutes') --> You can set processing time or time interval for each execution
		availableNow(OneTime) --> .trigger(availableNow=True) --> Consume all the available records from previous execution as an incremental batch.
		continuous-->trigger(continuous='5 seconds')--> To consume data for continuous mention time.
	
	##AutoLoader:
		In order to ingest the data from a cloud platform to lakehouse(Bronze--Silver--Gold), have to handel millions of data, schema changes, bad data eleminate..
		In short handel incremental load Data Engineer needs some complex logic to implement all of this.
		All this can be supported by using AutoLoader.
		
		* It's a optimize data ingestion tool that incrementally and efficiently process new data files as they arrive in the cloud storage built into the Databricks Lakehouse.
		* Auto Loader incrementally and efficiently process new data files as they arrive in cloud storage without any additional setup.
		* Can load data files from cloud storages without beign vendor specific(AWS S3, Azure ADLS, Google cloud Storage,DBFS).
		* Auto Loader can ingest JSON, CSV, PARQUET, AVRO, ORC, TEXT and BINARYFILE file formats.
		* This is beneficial when you are ingesting data into your lakehouse particularly into bronze layer as a streaming query.
	
	## Autoloader: Schema Inference:
		Implementing Autoloader:
			df_str = (spark.readStream
					.format('cloudFiles') ## This will tell the spark to use Autoloader
					.option('cloudFiles.format','csv') ##You can tell spark the input file format
					.option('header','true')
					.schema(schmea) ***
					.load(path))
		*** You can remove this bez auto loader fetch the schema use someting like schemaLocation 
				eg: .option('cloudFiles.SchmeaLocation',f'{stream_path}/schmeaInfer') --> it means it read the schema from the file and write that in mentioned location, Socket
					when ever it reads the next file it go to that location and compare the schema and fetch the schema from there.
		df_r_stream = (spark.readStream
               .format('cloudFiles')
               .option('cloudFiles.format','csv')
               .option('cloudFiles.SchmeaLocation',f'{stream_path}/schmeaInfer')
			   .option('cloudFiles.inferColumnTypes','true') #It fetch the data correct data type
               .option('header','true')
               .schema(schema).load(stream_path))
		#schemaHints:
			df_r_stream = (spark.readStream
               .format('cloudFiles')
               .option('cloudFiles.format','csv')
               .option('cloudFiles.SchemaLocation',f'{stream_path}/schmeaInfer')
               .option('cloudFiles.inferColumnTypes','true')
               .option('cloudFiles.schemaHints','Citizens Long') ***
               .option('header','true').load(stream_path))
			   
			*** Schema Hints is something, when ever this columns comes we have to apply the data types as Long
	
	##Schema Evolution:
		Schema evolution is the process of managing changes in data schema as it evolves over time, often due to updates in software or changing business requirements,
		which can cause schema drift.
		
		Ways to handel schema changes:
			* Failes the stream
			* Manually change the existing schema
			* Evolve automatically with change in schema
		
		Schema Evolution:
			* addNewColumns = Stream fails. New columns are added to the schema.Existing columns do not evolve data types.
			* failOnNewColumns = Stream fails. Stream does not restart unless the provided schema is updated, or the offending data file is removed
			* rescue  = Schema is never evolved and stream does not fail due to schema changes. All new columns are recorded in the rescued data column.
			* none = ignore any new columns (Does not evolve the schema, new columns are ignored, and data is not rescued unless the rescuedDataColumn option is set.
					(Stream does not fail due to schema changes)
	
	
##Project Setup:
	###Ingestion To Bronze:
		Deta Lake--> Bronze(Using Auto Loader, Trigger availableNow(Incremental batch processing))
		
	Note: Auto loader can ingest and process millions of files/ hours, It's by default perform increamental loadding.
		By maintaining metadata. 

	##Silver Transformation:
		*Read Bronze data(we are not using Auto Loader, It's only work on cloudFiles, If there is any source data in cloud storage and that is in the format of Files we can use Auto Loader, In our case we load data in delta tables))
		we are using delta format, spark stracture streaming will take care of increamental load.
		
		#### How we perform Data Quality check:
			## Drop duplicates data:
				@ dropDuplicates():
					df.dropDuplicates()
			## Replacing Null values:
				there is 2 scenarios:
					1. String columns
					2. Numeric Columns
					
					For string columns --> 'NA'
					For Numeric Columns --> 0/Null
					We can find the number of columns:
					columns = df.schema.names
					str:
						df_N_Str = df.fillna('Unknown',subset = columns)
					Numeric:
						df_N_Str = df.fillna(0,subset = columns)
					
					Note: Spark is quite intellegent to understand if there is a string column it'll replace that with string value and if it's numeric it'll replace with 0.
		##Count:
			df.withColumn('Electric_Vehicles_count', (col('ev_cars') + col('ev_bikes))
	##Gold Layer:
		Less transformation and Data is loaded to the final table to generate powerBI report
	
	We can create databricks workflow to automate the process(it means when ever a new file comes to the landing container notebooks will autometically start execute)
	Note: Databricks Workflow is more less similar with ADF
	
	In databricks workflow we have 3 triggers:
		1. Scheduled
		2. File Arrival
		3. Continuous
	Scheduled: Here we can scedule based on minutes, hours, day, week,month
				It further divided into 2
					1. simple: Runs periodically based on the specified time unit and interval(Hour, Day, Week)
					2. Cron Trigger: cron is a UNIX tool that has been around for a long time, so its scheduling capabilities are powerful and proven. 
						The CronTrigger class is based on the scheduling capabilities of cron
						example: “At 8:00am every Monday through Friday” or “At 1:30am every last Friday of the month”.
	
	File Arrival: File arrival triggers monitor cloud storage paths of up to 10,000 files for new files. These paths are either Unity Catalog volumes 
					or external locations managed through Unity Catalog.
				When ever a file is arrived at a define location the jobs will start executing
						NOTE: The storage location to check for file arrivals. This can be a volume, an external location, a subfolder of a volume or an external location.
						
	Continuous: A Continuous job is intended to be always running, and it ensure that there is always one active run.
				A new run is created as soon as the previous one finished(due to failure or success) or when there are none.
					Note: Task dependencies and retry policy are not alllowed for continous jobs.
	

###CICD:







	
## Delta Live Tsbles (DLT):
	##Origin of Delta Live tales:
	
	##Considerations in Lakehouse Architecture
	
	##Understanding Declarative ETL
	
	##Limitations of Delta Live Tables:
	
	##Defining Tables from datasets
	
	##{'CHKPoint':'checkpoints', 'CHKPointXT':'','ENV':'dev','Debug':1,'run_type':'NA'}
	